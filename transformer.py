# -*- coding: utf-8 -*-
"""Transformer2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f30HTHA7PhJ4jyx-b-i-fBJWqidYYOpA
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
url = 'https://raw.githubusercontent.com/saqib3171/ADML---Climate-Parameters-Forecasting-/main/DailyDelhiClimateTrain.csv'
df1 = pd.read_csv(url)
df1 = df1.set_index('date')

print(df1)

from sklearn.ensemble import IsolationForest

# Check for outliers
target_column = 'humidity'

def detect_and_impute_outliers(data, target_column):
    # Ensure target column exists in the data
    if target_column not in data.columns:
        raise ValueError(f"Column '{target_column}' not found in the data.")

    # Reshape data for IsolationForest
    values = data[target_column].values.reshape(-1, 1)

    # Initialize and fit the model
    iso_forest = IsolationForest(contamination=0.05)  # Estimate of the proportion of outliers
    outliers = iso_forest.fit_predict(values)

    # Convert outliers to a boolean mask
    outlier_mask = outliers == -1

    # Impute outliers with the median of the non-outlier values
    median_value = np.median(data.loc[~outlier_mask, target_column])
    data.loc[outlier_mask, target_column] = median_value

    return data
    # Apply outlier detection and imputation
df1 = detect_and_impute_outliers(df1, target_column)

# Display the DataFrame with imputed values
print(df1)

# Ensure that 'date' is a datetime object
df1['date'] = pd.to_datetime(df1.index)

# Extract day and month
df1['day_of_week'] = df1['date'].dt.day_name()
df1['month'] = df1['date'].dt.month_name()

# Create a pivot table
pivot = df1.pivot_table(values='humidity', index='day_of_week', columns='month', aggfunc='mean')
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
sns.heatmap(pivot, annot=True, cmap='coolwarm', fmt=".1f")
plt.title('Average Humidity by Day of Week and Month')
plt.ylabel('Day of Week')
plt.xlabel('Month')
plt.show()

from sklearn.model_selection import TimeSeriesSplit

T = 10
X = []
Y = []

target_column = 'humidity'
series = df1[target_column]

for t in range(len(series) - T):
    x = series[t:t+T]
    X.append(x)
    y = series[t+T]
    Y.append(y)

X = np.array(X).reshape(-1,T)
Y = np.array(Y)
N = len(X)

print(X)
print(Y)

!pip install transformers

!pip install -q transformers

!pip install -q datasets

!pip install -q evaluate

!pip install -q accelerate

!pip install -q gluonts ujson

from datasets import load_dataset

##

from datasets import Dataset, DatasetDict

df1.index = pd.to_datetime(df1.index)

# Resample the data by month and get the values for each month
monthly_data = df1.resample('M').agg(lambda x: x.tolist())

# Calculate the index to split the data into train and validation (80-20 split)
split_index = int(31/2)
split_val = int(31/2 + 31/4)


# Create 'train' and 'validation' dictionaries
train_dataset = {
    'start': monthly_data.index,
    'target': [month[:split_val] for month in monthly_data['humidity']],
    'item_id': list(range(1, len(monthly_data) + 1)),
    'feat_dynamic_real': len(monthly_data)*[None],
    'feat_static_cat': list(range(1, len(monthly_data) + 1))
}

validation_dataset = {
    'start': monthly_data.index,
    'target': monthly_data['humidity'],
    'item_id': list(range(1, len(monthly_data) + 1)),
    'feat_dynamic_real': len(monthly_data)*[None],
    'feat_static_cat': list(range(1, len(monthly_data) + 1))
}

test_dataset = {
    'start': monthly_data.index,
    'target': monthly_data['humidity'],
    'item_id': list(range(1, len(monthly_data) + 1)),
    'feat_dynamic_real': len(monthly_data)*[None],
    'feat_static_cat': list(range(1, len(monthly_data) + 1))
}

# Create Dataset objects
train_dataset = Dataset.from_dict(train_dataset)
test_dataset = Dataset.from_dict(test_dataset)
validation_dataset = Dataset.from_dict(validation_dataset)

# Create the DatasetDict
dataset = DatasetDict({
    'train': train_dataset,
    'validation': validation_dataset,
    'test': test_dataset
})

# Display or use the dataset_dict as needed
print(dataset)

dataset

validation_example = dataset['validation'][0]
train_example = dataset['train'][0]
validation_example.keys()

print(validation_example['start'])
print(validation_example['target'])
print(train_example['start'])
print(train_example['target'])

freq = "1D"
prediction_length = int(31/4)+1

assert len(train_example["target"]) + prediction_length == len(
    validation_example["target"]
)

import matplotlib.pyplot as plt

figure, axes = plt.subplots()
axes.plot(train_example["target"], color="blue")
axes.plot(validation_example["target"], color="red", alpha=0.5)

plt.show()

train_dataset = dataset["train"]
test_dataset = dataset["test"]

from functools import lru_cache

import pandas as pd
import numpy as np


def convert_to_pandas_period(date, freq):
    return pd.Period(date, freq)

def transform_start_field(batch, freq):
    batch["start"] = [convert_to_pandas_period(date, freq) for date in batch["start"]]
    return batch

from functools import partial

train_dataset.set_transform(partial(transform_start_field, freq=freq))
test_dataset.set_transform(partial(transform_start_field, freq=freq))

from gluonts.time_feature import get_lags_for_frequency

lags_sequence = get_lags_for_frequency(freq)
print(lags_sequence)

from gluonts.time_feature import time_features_from_frequency_str

time_features = time_features_from_frequency_str(freq)
print(time_features)

from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction

config = TimeSeriesTransformerConfig(
    prediction_length=prediction_length,
    # context length:
    context_length=prediction_length * 2,
    # lags coming from helper given the freq:
    lags_sequence=lags_sequence,
    # we'll add 2 time features ("month of year" and "age", see further):
    num_time_features=len(time_features) + 1,
    # we have a single static categorical feature, namely time series ID:
    num_static_categorical_features=0,
    # it has 366 possible values:
    cardinality=[len(train_dataset)],
    # the model will learn an embedding of size 2 for each of the 366 possible values:
    embedding_dimension=[2],

    # transformer params:
    encoder_layers=4,
    decoder_layers=8,
    d_model=32,
)

model = TimeSeriesTransformerForPrediction(config)

model.config.distribution_output

from gluonts.time_feature import (
    time_features_from_frequency_str,
    TimeFeature,
    get_lags_for_frequency,
)
from gluonts.dataset.field_names import FieldName
from gluonts.transform import (
    AddAgeFeature,
    AddObservedValuesIndicator,
    AddTimeFeatures,
    AsNumpyArray,
    Chain,
    ExpectedNumInstanceSampler,
    InstanceSplitter,
    RemoveFields,
    SelectFields,
    SetField,
    TestSplitSampler,
    Transformation,
    ValidationSplitSampler,
    VstackFeatures,
    RenameFields,
)

from transformers import PretrainedConfig

def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:
    remove_field_names = []
    if config.num_static_real_features == 0:
        remove_field_names.append(FieldName.FEAT_STATIC_REAL)
    if config.num_dynamic_real_features == 0:
        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)
    if config.num_static_categorical_features == 0:
        remove_field_names.append(FieldName.FEAT_STATIC_CAT)

    # a bit like torchvision.transforms.Compose
    return Chain(
        # step 1: remove static/dynamic fields if not specified
        [RemoveFields(field_names=remove_field_names)]
        # step 2: convert the data to NumPy (potentially not needed)
        + (
            [
                AsNumpyArray(
                    field=FieldName.FEAT_STATIC_CAT,
                    expected_ndim=1,
                    dtype=int,
                )
            ]
            if config.num_static_categorical_features > 0
            else []
        )
        + (
            [
                AsNumpyArray(
                    field=FieldName.FEAT_STATIC_REAL,
                    expected_ndim=1,
                )
            ]
            if config.num_static_real_features > 0
            else []
        )
        + [
            AsNumpyArray(
                field=FieldName.TARGET,
                # we expect an extra dim for the multivariate case:
                expected_ndim=1 if config.input_size == 1 else 2,
            ),
            # step 3: handle the NaN's by filling in the target with zero
            # and return the mask (which is in the observed values)
            # true for observed values, false for nan's
            # the decoder uses this mask (no loss is incurred for unobserved values)
            # see loss_weights inside the xxxForPrediction model
            AddObservedValuesIndicator(
                target_field=FieldName.TARGET,
                output_field=FieldName.OBSERVED_VALUES,
            ),
            # step 4: add temporal features based on freq of the dataset
            # month of year in the case when freq="M"
            # these serve as positional encodings
            AddTimeFeatures(
                start_field=FieldName.START,
                target_field=FieldName.TARGET,
                output_field=FieldName.FEAT_TIME,
                time_features=time_features_from_frequency_str(freq),
                pred_length=config.prediction_length,
            ),
            # step 5: add another temporal feature (just a single number)
            # tells the model where in its life the value of the time series is,
            # sort of a running counter
            AddAgeFeature(
                target_field=FieldName.TARGET,
                output_field=FieldName.FEAT_AGE,
                pred_length=config.prediction_length,
                log_scale=True,
            ),
            # step 6: vertically stack all the temporal features into the key FEAT_TIME
            VstackFeatures(
                output_field=FieldName.FEAT_TIME,
                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]
                + (
                    [FieldName.FEAT_DYNAMIC_REAL]
                    if config.num_dynamic_real_features > 0
                    else []
                ),
            ),
            # step 7: rename to match HuggingFace names
            RenameFields(
                mapping={
                    FieldName.FEAT_STATIC_CAT: "static_categorical_features",
                    FieldName.FEAT_STATIC_REAL: "static_real_features",
                    FieldName.FEAT_TIME: "time_features",
                    FieldName.TARGET: "values",
                    FieldName.OBSERVED_VALUES: "observed_mask",
                }
            ),
        ]
    )

from gluonts.transform.sampler import InstanceSampler
from typing import Optional

def create_instance_splitter(
    config: PretrainedConfig,
    mode: str,
    train_sampler: Optional[InstanceSampler] = None,
    validation_sampler: Optional[InstanceSampler] = None,
) -> Transformation:
    assert mode in ["train", "validation", "test"]

    instance_sampler = {
        "train": train_sampler
        or ExpectedNumInstanceSampler(
            num_instances=1.0, min_future=config.prediction_length
        ),
        "validation": validation_sampler
        or ValidationSplitSampler(min_future=config.prediction_length),
        "test": TestSplitSampler(),
    }[mode]

    return InstanceSplitter(
        target_field="values",
        is_pad_field=FieldName.IS_PAD,
        start_field=FieldName.START,
        forecast_start_field=FieldName.FORECAST_START,
        instance_sampler=instance_sampler,
        past_length=config.context_length + max(config.lags_sequence),
        future_length=config.prediction_length,
        time_series_fields=["time_features", "observed_mask"],
    )

from typing import Iterable

import torch
from gluonts.itertools import Cached, Cyclic
from gluonts.dataset.loader import as_stacked_batches


def create_train_dataloader(
    config: PretrainedConfig,
    freq,
    data,
    batch_size: int,
    num_batches_per_epoch: int,
    shuffle_buffer_length: Optional[int] = None,
    cache_data: bool = True,
    **kwargs,
) -> Iterable:
    PREDICTION_INPUT_NAMES = [
        "past_time_features",
        "past_values",
        "past_observed_mask",
        "future_time_features",
    ]
    if config.num_static_categorical_features > 0:
        PREDICTION_INPUT_NAMES.append("static_categorical_features")

    if config.num_static_real_features > 0:
        PREDICTION_INPUT_NAMES.append("static_real_features")

    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [
        "future_values",
        "future_observed_mask",
    ]

    transformation = create_transformation(freq, config)
    transformed_data = transformation.apply(data, is_train=True)
    if cache_data:
        transformed_data = Cached(transformed_data)

    # we initialize a Training instance
    instance_splitter = create_instance_splitter(config, "train")

    # the instance splitter will sample a window of
    # context length + lags + prediction length (from the 366 possible transformed time series)
    # randomly from within the target time series and return an iterator.
    stream = Cyclic(transformed_data).stream()
    training_instances = instance_splitter.apply(
        stream, is_train=True
    )

    return as_stacked_batches(
        training_instances,
        batch_size=batch_size,
        shuffle_buffer_length=shuffle_buffer_length,
        field_names=TRAINING_INPUT_NAMES,
        output_type=torch.tensor,
        num_batches_per_epoch=num_batches_per_epoch,
    )

def create_test_dataloader(
    config: PretrainedConfig,
    freq,
    data,
    batch_size: int,
    **kwargs,
):
    PREDICTION_INPUT_NAMES = [
        "past_time_features",
        "past_values",
        "past_observed_mask",
        "future_time_features",
    ]
    if config.num_static_categorical_features > 0:
        PREDICTION_INPUT_NAMES.append("static_categorical_features")

    if config.num_static_real_features > 0:
        PREDICTION_INPUT_NAMES.append("static_real_features")

    transformation = create_transformation(freq, config)
    transformed_data = transformation.apply(data, is_train=False)

    # we create a Test Instance splitter which will sample the very last
    # context window seen during training only for the encoder.
    instance_sampler = create_instance_splitter(config, "test")

    # we apply the transformations in test mode
    testing_instances = instance_sampler.apply(transformed_data, is_train=False)

    return as_stacked_batches(
        testing_instances,
        batch_size=batch_size,
        output_type=torch.tensor,
        field_names=PREDICTION_INPUT_NAMES,
    )

train_dataloader = create_train_dataloader(
    config=config,
    freq=freq,
    data=train_dataset,
    batch_size=64,
    num_batches_per_epoch=100,
)

test_dataloader = create_test_dataloader(
    config=config,
    freq=freq,
    data=test_dataset,
    batch_size=64,
)

batch = next(iter(train_dataloader))
for k, v in batch.items():
    print(k, v.shape, v.type())

# perform forward pass
outputs = model(
    past_values=batch["past_values"],
    past_time_features=batch["past_time_features"],
    past_observed_mask=batch["past_observed_mask"],
    static_categorical_features=batch["static_categorical_features"]
    if config.num_static_categorical_features > 0
    else None,
    static_real_features=batch["static_real_features"]
    if config.num_static_real_features > 0
    else None,
    future_values=batch["future_values"],
    future_time_features=batch["future_time_features"],
    future_observed_mask=batch["future_observed_mask"],
    output_hidden_states=True,
)

print("Loss:", outputs.loss.item())

from accelerate import Accelerator
from torch.optim import AdamW

accelerator = Accelerator()
device = accelerator.device

model.to(device)
optimizer = AdamW(model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)

model, optimizer, train_dataloader = accelerator.prepare(
    model,
    optimizer,
    train_dataloader,
)

model.train()
for epoch in range(40):
    for idx, batch in enumerate(train_dataloader):
        optimizer.zero_grad()

        # Ensure the input tensors have the correct data type
        static_categorical_features = (
            batch["static_categorical_features"]
            .to(device)
            .to(torch.float)  # Convert to float if not already
            if config.num_static_categorical_features > 0
            else None
        )

        static_real_features = (
            batch["static_real_features"]
            .to(device)
            .to(torch.float)  # Convert to float if not already
            if config.num_static_real_features > 0
            else None
        )

        past_time_features = batch["past_time_features"].to(device)
        past_values = batch["past_values"].to(device)
        future_time_features = batch["future_time_features"].to(device)
        future_values = batch["future_values"].to(device)
        past_observed_mask = batch["past_observed_mask"].to(device)
        future_observed_mask = batch["future_observed_mask"].to(device)

        outputs = model(
            static_categorical_features=static_categorical_features,
            static_real_features=static_real_features,
            past_time_features=past_time_features,
            past_values=past_values,
            future_time_features=future_time_features,
            future_values=future_values,
            past_observed_mask=past_observed_mask,
            future_observed_mask=future_observed_mask,
        )
        loss = outputs.loss

        # Backpropagation
        accelerator.backward(loss)
        optimizer.step()

        if idx % 100 == 0:
            print(loss.item())

model.eval()

forecasts = []

for batch in test_dataloader:
    outputs = model.generate(
        static_categorical_features=batch["static_categorical_features"].to(device)
        if config.num_static_categorical_features > 0
        else None,
        static_real_features=batch["static_real_features"].to(device)
        if config.num_static_real_features > 0
        else None,
        past_time_features=batch["past_time_features"].to(device),
        past_values=batch["past_values"].to(device),
        future_time_features=batch["future_time_features"].to(device),
        past_observed_mask=batch["past_observed_mask"].to(device),
    )
    forecasts.append(outputs.sequences.cpu().numpy())

forecasts[0].shape

forecasts = np.vstack(forecasts)
print(forecasts.shape)

from evaluate import load
from gluonts.time_feature import get_seasonality

mase_metric = load("evaluate-metric/mase")
smape_metric = load("evaluate-metric/smape")

forecast_median = np.median(forecasts, 1)

import matplotlib.dates as mdates


def plot(ts_index):
    fig, ax = plt.subplots()

    index = pd.period_range(
        start=validation_dataset[ts_index][FieldName.START],
        periods=len(validation_dataset[ts_index][FieldName.TARGET]),
        freq=freq,
    ).to_timestamp()


    # Major ticks every day
    ax.xaxis.set_major_locator(mdates.DayLocator())
    # Minor ticks every 6 hours
    ax.xaxis.set_minor_locator(mdates.HourLocator(byhour=range(0, 24, 6)))
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%d'))

    ax.plot(
        index,
        validation_dataset[ts_index]["target"],
        label="actual",
    )

    plt.plot(
        index[-prediction_length:],
        np.median(forecasts[ts_index], axis=0),
        label="median",
    )

    plt.fill_between(
        index[-prediction_length:],
        forecasts[ts_index].mean(0) - forecasts[ts_index].std(axis=0),
        forecasts[ts_index].mean(0) + forecasts[ts_index].std(axis=0),
        alpha=0.3,
        interpolate=True,
        label="+/- 1-std",
    )
    plt.legend()
    plt.show()

# Plot of Original Time Series Data
plt.figure(figsize=(10, 6))
plt.plot(df1.index, df1['humidity'], label='Humidity')
plt.xlabel('Date')
plt.ylabel('Humidity')
plt.title('Time Series of Humidity')
plt.legend()
plt.show()

# Plot of Train and Validation Examples
plt.figure(figsize=(10, 6))
plt.plot(train_example["target"], label='Train', color='blue')
plt.plot(range(len(train_example["target"]), len(train_example["target"]) + len(validation_example["target"])), validation_example["target"], label='Validation', color='red', alpha=0.7)
plt.xlabel('Time Steps')
plt.ylabel('Humidity')
plt.title('Train and Validation Series')
plt.legend()
plt.show()

# Comparison of Actual vs. Forecasted Data
def plot_forecast(ts_index):
    actual = validation_dataset[ts_index]['target']
    forecast = forecasts[ts_index]

    plt.figure(figsize=(10, 6))
    plt.plot(actual, label='Actual', color='blue')
    plt.plot(range(len(actual) - len(forecast), len(actual)), forecast, label='Forecast', color='red')
    plt.xlabel('Time Steps')
    plt.ylabel('Humidity')
    plt.title(f'Actual vs Forecast for Time Series {ts_index}')
    plt.legend()
    plt.show()

plot_forecast(0)  # choose index of the time series wish to plot

plt.figure(figsize=(10, 6))
plt.hist(forecast_median.flatten(), bins=20, color='skyblue', edgecolor='black')
plt.xlabel('Forecasted Humidity')
plt.ylabel('Frequency')
plt.title('Distribution of Forecasted Values')
plt.show()

# Test model with given test data file
from datasets import Dataset, DatasetDict
from functools import partial

# Function to convert date to pandas period
def convert_to_pandas_period(date, freq):
    return pd.Period(date, freq)

# Function to transform start field
def transform_start_field(batch, freq):
    batch["start"] = [convert_to_pandas_period(date, freq) for date in batch["start"]]
    return batch

# 1. Load the Test Data
test_url = 'https://raw.githubusercontent.com/saqib3171/ADML---Climate-Parameters-Forecasting-/main/DailyDelhiClimateTest.csv'
df_test = pd.read_csv(test_url)
df_test = df_test.set_index('date')
df_test.index = pd.to_datetime(df_test.index)

# 2. Preprocess the Test Data
monthly_test_data = df_test.resample('M').apply(lambda x: x.tolist())

# Create 'test' dictionary
test_dataset = {
    'start': monthly_test_data.index,
    'target': [month.humidity for month in monthly_test_data.itertuples()],
    'item_id': list(range(1, len(monthly_test_data) + 1)),
    'feat_dynamic_real': len(monthly_test_data) * [None],
    'feat_static_cat': list(range(1, len(monthly_test_data) + 1))
}

# Create Dataset object
test_dataset = Dataset.from_dict(test_dataset)


# Transform the test dataset
freq = "1D"
test_dataset.set_transform(partial(transform_start_field, freq=freq))

# 3. Create a DataLoader for the Test Dataset
test_dataloader = create_test_dataloader(
    config=config,
    freq=freq,
    data=test_dataset,
    batch_size=64
)

# 4. Predict Using the Model
model.eval()

test_forecasts = []

for batch in test_dataloader:
    outputs = model.generate(
        static_categorical_features=batch["static_categorical_features"].to(device)
        if config.num_static_categorical_features > 0
        else None,
        static_real_features=batch["static_real_features"].to(device)
        if config.num_static_real_features > 0
        else None,
        past_time_features=batch["past_time_features"].to(device),
        past_values=batch["past_values"].to(device),
        future_time_features=batch["future_time_features"].to(device),
        past_observed_mask=batch["past_observed_mask"].to(device),
    )
    test_forecasts.append(outputs.sequences.cpu().numpy())

test_forecasts = np.vstack(test_forecasts)

# Display the forecast results
print(test_forecasts)

from sklearn.metrics import mean_absolute_error
import math

# Function to get actual values for the validation dataset
def get_actual_values(validation_dataset):
    # Extract the actual values for the validation dataset's prediction period
    actual_values = [series['target'] for series in validation_dataset]
    return np.concatenate(actual_values)

# Get actual values for the validation dataset
actual_values_flat = get_actual_values(validation_dataset)

# Extract the relevant slice from the forecasts for validation
# Assuming we are interested in the first feature of the first time series
forecasts_slice = forecasts[:, :, 0]  # Adjust indices as required

# Flatten the slice for comparison
forecasts_flat = forecasts_slice.flatten()

# Ensure the length of forecasts matches the length of actual values
if len(forecasts_flat) > len(actual_values_flat):
    forecasts_flat = forecasts_flat[:len(actual_values_flat)]

# Calculate the error metrics
if len(actual_values_flat) == len(forecasts_flat):
    # Compute Mean Absolute Error (MAE)
    mae = mean_absolute_error(actual_values_flat, forecasts_flat)
    print(f"Mean Absolute Error (MAE) on Validation Data: {mae}")

from sklearn.metrics import mean_absolute_error, mean_squared_error
import math

# Function to get actual values for the test dataset
def get_actual_values(test_dataset):
    # Extract the actual values for the test dataset's prediction period
    actual_values = [series['target'][-prediction_length:] for series in test_dataset]
    return np.concatenate(actual_values)

# Get actual values for the test dataset
actual_values_flat = get_actual_values(test_dataset)

# Extract the relevant slice from the forecasts
# Assuming we are interested in the first feature of the first time series
forecasts_slice = test_forecasts[0, :, 0]  # we can adjust indices as required

# Flatten the slice for comparison
forecasts_flat = forecasts_slice.flatten()

# Ensure the length of forecasts matches the length of actual values
if len(forecasts_flat) > len(actual_values_flat):
   forecasts_flat = forecasts_flat[:len(actual_values_flat)]

# Calculate the error metrics
if len(actual_values_flat) == len(forecasts_flat):
    # Compute Mean Absolute Error (MAE)
    mae = mean_absolute_error(actual_values_flat, forecasts_flat)
    print(f"Mean Absolute Error (MAE): {mae}")